docker run --gpus all -it --rm \
-v `pwd`:/home/user/workdir \
ghcr.io/pinto0309/openvino2tensorflow:latest

MODEL=mirnet

H=180
W=320
saved_model_to_tflite \
--saved_model_dir_path saved_model_${H}x${W} \
--output_no_quant_float32_tflite \
--output_dynamic_range_quant_tflite \
--output_weight_quant_tflite \
--output_float16_quant_tflite \
--output_tfjs \
--output_coreml
mv tflite_from_saved_model/* saved_model_${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_${H}x${W}/model_float32.tflite \
--output saved_model_${H}x${W}/model_float32.onnx
onnxsim saved_model_${H}x${W}/model_float32.onnx saved_model_${H}x${W}/model_float32.onnx
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model saved_model_${H}x${W}/model_float32.onnx \
--data_type FP32 \
--output_dir saved_model_${H}x${W}/openvino/FP32 \
--model_name ${MODEL}_${H}x${W}
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model saved_model_${H}x${W}/model_float32.onnx \
--data_type FP16 \
--output_dir saved_model_${H}x${W}/openvino/FP16 \
--model_name ${MODEL}_${H}x${W}

H=240
W=320
saved_model_to_tflite \
--saved_model_dir_path saved_model_${H}x${W} \
--output_no_quant_float32_tflite \
--output_dynamic_range_quant_tflite \
--output_weight_quant_tflite \
--output_float16_quant_tflite \
--output_tfjs \
--output_coreml
mv tflite_from_saved_model/* saved_model_${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_${H}x${W}/model_float32.tflite \
--output saved_model_${H}x${W}/model_float32.onnx
onnxsim saved_model_${H}x${W}/model_float32.onnx saved_model_${H}x${W}/model_float32.onnx
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model saved_model_${H}x${W}/model_float32.onnx \
--data_type FP32 \
--output_dir saved_model_${H}x${W}/openvino/FP32 \
--model_name ${MODEL}_${H}x${W}
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model saved_model_${H}x${W}/model_float32.onnx \
--data_type FP16 \
--output_dir saved_model_${H}x${W}/openvino/FP16 \
--model_name ${MODEL}_${H}x${W}

H=240
W=480
saved_model_to_tflite \
--saved_model_dir_path saved_model_${H}x${W} \
--output_no_quant_float32_tflite \
--output_dynamic_range_quant_tflite \
--output_weight_quant_tflite \
--output_float16_quant_tflite \
--output_tfjs \
--output_coreml
mv tflite_from_saved_model/* saved_model_${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_${H}x${W}/model_float32.tflite \
--output saved_model_${H}x${W}/model_float32.onnx
onnxsim saved_model_${H}x${W}/model_float32.onnx saved_model_${H}x${W}/model_float32.onnx
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model saved_model_${H}x${W}/model_float32.onnx \
--data_type FP32 \
--output_dir saved_model_${H}x${W}/openvino/FP32 \
--model_name ${MODEL}_${H}x${W}
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model saved_model_${H}x${W}/model_float32.onnx \
--data_type FP16 \
--output_dir saved_model_${H}x${W}/openvino/FP16 \
--model_name ${MODEL}_${H}x${W}

H=360
W=480
saved_model_to_tflite \
--saved_model_dir_path saved_model_${H}x${W} \
--output_no_quant_float32_tflite \
--output_dynamic_range_quant_tflite \
--output_weight_quant_tflite \
--output_float16_quant_tflite \
--output_tfjs \
--output_coreml
mv tflite_from_saved_model/* saved_model_${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_${H}x${W}/model_float32.tflite \
--output saved_model_${H}x${W}/model_float32.onnx
onnxsim saved_model_${H}x${W}/model_float32.onnx saved_model_${H}x${W}/model_float32.onnx
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model saved_model_${H}x${W}/model_float32.onnx \
--data_type FP32 \
--output_dir saved_model_${H}x${W}/openvino/FP32 \
--model_name ${MODEL}_${H}x${W}
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model saved_model_${H}x${W}/model_float32.onnx \
--data_type FP16 \
--output_dir saved_model_${H}x${W}/openvino/FP16 \
--model_name ${MODEL}_${H}x${W}

H=360
W=640
saved_model_to_tflite \
--saved_model_dir_path saved_model_${H}x${W} \
--output_no_quant_float32_tflite \
--output_dynamic_range_quant_tflite \
--output_weight_quant_tflite \
--output_float16_quant_tflite \
--output_tfjs \
--output_coreml
mv tflite_from_saved_model/* saved_model_${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_${H}x${W}/model_float32.tflite \
--output saved_model_${H}x${W}/model_float32.onnx
onnxsim saved_model_${H}x${W}/model_float32.onnx saved_model_${H}x${W}/model_float32.onnx
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model saved_model_${H}x${W}/model_float32.onnx \
--data_type FP32 \
--output_dir saved_model_${H}x${W}/openvino/FP32 \
--model_name ${MODEL}_${H}x${W}
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model saved_model_${H}x${W}/model_float32.onnx \
--data_type FP16 \
--output_dir saved_model_${H}x${W}/openvino/FP16 \
--model_name ${MODEL}_${H}x${W}

H=480
W=640
saved_model_to_tflite \
--saved_model_dir_path saved_model_${H}x${W} \
--output_no_quant_float32_tflite \
--output_dynamic_range_quant_tflite \
--output_weight_quant_tflite \
--output_float16_quant_tflite \
--output_tfjs \
--output_coreml
mv tflite_from_saved_model/* saved_model_${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_${H}x${W}/model_float32.tflite \
--output saved_model_${H}x${W}/model_float32.onnx
onnxsim saved_model_${H}x${W}/model_float32.onnx saved_model_${H}x${W}/model_float32.onnx
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model saved_model_${H}x${W}/model_float32.onnx \
--data_type FP32 \
--output_dir saved_model_${H}x${W}/openvino/FP32 \
--model_name ${MODEL}_${H}x${W}
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model saved_model_${H}x${W}/model_float32.onnx \
--data_type FP16 \
--output_dir saved_model_${H}x${W}/openvino/FP16 \
--model_name ${MODEL}_${H}x${W}

H=720
W=1280
saved_model_to_tflite \
--saved_model_dir_path saved_model_${H}x${W} \
--output_no_quant_float32_tflite \
--output_dynamic_range_quant_tflite \
--output_weight_quant_tflite \
--output_float16_quant_tflite \
--output_tfjs \
--output_coreml
mv tflite_from_saved_model/* saved_model_${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_${H}x${W}/model_float32.tflite \
--output saved_model_${H}x${W}/model_float32.onnx
onnxsim saved_model_${H}x${W}/model_float32.onnx saved_model_${H}x${W}/model_float32.onnx
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model saved_model_${H}x${W}/model_float32.onnx \
--data_type FP32 \
--output_dir saved_model_${H}x${W}/openvino/FP32 \
--model_name ${MODEL}_${H}x${W}
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model saved_model_${H}x${W}/model_float32.onnx \
--data_type FP16 \
--output_dir saved_model_${H}x${W}/openvino/FP16 \
--model_name ${MODEL}_${H}x${W}




python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--saved-model saved_model_HxW \
--output saved_model_HxW/model_float32.onnx

H=40
W=40
saved_model_to_tflite \
--saved_model_dir_path saved_model_Nx${H}x${W} \
--output_no_quant_float32_tflite
mv tflite_from_saved_model/* saved_model_Nx${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_Nx${H}x${W}/model_float32.tflite \
--output saved_model_Nx${H}x${W}/model_float32.onnx

H=80
W=80
saved_model_to_tflite \
--saved_model_dir_path saved_model_Nx${H}x${W} \
--output_no_quant_float32_tflite
mv tflite_from_saved_model/* saved_model_Nx${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_Nx${H}x${W}/model_float32.tflite \
--output saved_model_Nx${H}x${W}/model_float32.onnx

H=120
W=120
saved_model_to_tflite \
--saved_model_dir_path saved_model_Nx${H}x${W} \
--output_no_quant_float32_tflite
mv tflite_from_saved_model/* saved_model_Nx${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_Nx${H}x${W}/model_float32.tflite \
--output saved_model_Nx${H}x${W}/model_float32.onnx

H=120
W=160
saved_model_to_tflite \
--saved_model_dir_path saved_model_Nx${H}x${W} \
--output_no_quant_float32_tflite
mv tflite_from_saved_model/* saved_model_Nx${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_Nx${H}x${W}/model_float32.tflite \
--output saved_model_Nx${H}x${W}/model_float32.onnx

H=120
W=320
saved_model_to_tflite \
--saved_model_dir_path saved_model_Nx${H}x${W} \
--output_no_quant_float32_tflite
mv tflite_from_saved_model/* saved_model_Nx${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_Nx${H}x${W}/model_float32.tflite \
--output saved_model_Nx${H}x${W}/model_float32.onnx

H=120
W=480
saved_model_to_tflite \
--saved_model_dir_path saved_model_Nx${H}x${W} \
--output_no_quant_float32_tflite
mv tflite_from_saved_model/* saved_model_Nx${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_Nx${H}x${W}/model_float32.tflite \
--output saved_model_Nx${H}x${W}/model_float32.onnx

H=120
W=640
saved_model_to_tflite \
--saved_model_dir_path saved_model_Nx${H}x${W} \
--output_no_quant_float32_tflite
mv tflite_from_saved_model/* saved_model_Nx${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_Nx${H}x${W}/model_float32.tflite \
--output saved_model_Nx${H}x${W}/model_float32.onnx

H=120
W=1280
saved_model_to_tflite \
--saved_model_dir_path saved_model_Nx${H}x${W} \
--output_no_quant_float32_tflite
mv tflite_from_saved_model/* saved_model_Nx${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_Nx${H}x${W}/model_float32.tflite \
--output saved_model_Nx${H}x${W}/model_float32.onnx

H=180
W=480
saved_model_to_tflite \
--saved_model_dir_path saved_model_Nx${H}x${W} \
--output_no_quant_float32_tflite
mv tflite_from_saved_model/* saved_model_Nx${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_Nx${H}x${W}/model_float32.tflite \
--output saved_model_Nx${H}x${W}/model_float32.onnx

H=180
W=640
saved_model_to_tflite \
--saved_model_dir_path saved_model_Nx${H}x${W} \
--output_no_quant_float32_tflite
mv tflite_from_saved_model/* saved_model_Nx${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_Nx${H}x${W}/model_float32.tflite \
--output saved_model_Nx${H}x${W}/model_float32.onnx

H=180
W=1280
saved_model_to_tflite \
--saved_model_dir_path saved_model_Nx${H}x${W} \
--output_no_quant_float32_tflite
mv tflite_from_saved_model/* saved_model_Nx${H}x${W}
rm -rf tflite_from_saved_model
python -m tf2onnx.convert \
--opset 11 \
--inputs-as-nchw input \
--tflite saved_model_Nx${H}x${W}/model_float32.tflite \
--output saved_model_Nx${H}x${W}/model_float32.onnx



python3 shape_inference.py saved_model_HxW/model_float32.onnx
python3 shape_inference.py saved_model_Nx40x40/model_float32.onnx
python3 shape_inference.py saved_model_Nx80x80/model_float32.onnx
python3 shape_inference.py saved_model_Nx120x120/model_float32.onnx
python3 shape_inference.py saved_model_Nx120x160/model_float32.onnx
python3 shape_inference.py saved_model_Nx120x320/model_float32.onnx
python3 shape_inference.py saved_model_Nx120x480/model_float32.onnx
python3 shape_inference.py saved_model_Nx120x640/model_float32.onnx
python3 shape_inference.py saved_model_Nx120x1280/model_float32.onnx
python3 shape_inference.py saved_model_Nx180x480/model_float32.onnx
python3 shape_inference.py saved_model_Nx180x640/model_float32.onnx
python3 shape_inference.py saved_model_Nx180x1280/model_float32.onnx
