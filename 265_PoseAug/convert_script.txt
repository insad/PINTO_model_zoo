docker run --gpus all -it --rm \
-v `pwd`:/home/user/workdir \
ghcr.io/pinto0309/openvino2tensorflow:latest

python3 batchsize_clear.py pauseaug_gcn_1x16x2.onnx pauseaug_gcn_Nx16x2.onnx
python3 batchsize_clear.py pauseaug_mlp_1x16x2.onnx pauseaug_mlp_Nx16x2.onnx
python3 batchsize_clear.py pauseaug_videopose_1x16x2.onnx pauseaug_videopose_Nx16x2.onnx

for i in {1..10} ; do
    python3 set_static_shape.py --model pauseaug_gcn_Nx16x2.onnx --batch_size ${i}
done
for i in {1..10} ; do
    python3 set_static_shape.py --model pauseaug_mlp_Nx16x2.onnx --batch_size ${i}
done

MODEL=pauseaug_gcn
for BATCH_SIZE in {1..10} ; do
    $INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
    --input_model ${MODEL}_${BATCH_SIZE}x16x2.onnx \
    --data_type FP32 \
    --output_dir ${MODEL}_${BATCH_SIZE}x16x2/openvino/FP32 \
    --model_name ${MODEL}_${BATCH_SIZE}x16x2
    $INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
    --input_model ${MODEL}_${BATCH_SIZE}x16x2.onnx \
    --data_type FP16 \
    --output_dir ${MODEL}_${BATCH_SIZE}x16x2/openvino/FP16 \
    --model_name ${MODEL}_${BATCH_SIZE}x16x2
    mkdir -p ${MODEL}_${BATCH_SIZE}x16x2/openvino/myriad
    ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/lib/intel64/myriad_compile \
    -m ${MODEL}_${BATCH_SIZE}x16x2/openvino/FP16/${MODEL}_${BATCH_SIZE}x16x2.xml \
    -ip U8 \
    -VPU_NUMBER_OF_SHAVES 4 \
    -VPU_NUMBER_OF_CMX_SLICES 4 \
    -o ${MODEL}_${BATCH_SIZE}x16x2/openvino/myriad/${MODEL}_${BATCH_SIZE}x16x2.blob
    openvino2tensorflow \
    --model_path ${MODEL}_${BATCH_SIZE}x16x2/openvino/FP32/${MODEL}_${BATCH_SIZE}x16x2.xml \
    --output_saved_model \
    --output_pb \
    --output_no_quant_float32_tflite \
    --output_dynamic_range_quant_tflite \
    --output_weight_quant_tflite \
    --output_float16_quant_tflite \
    --output_integer_quant_tflite \
    --output_integer_quant_typ 'uint8' \
    --string_formulas_for_normalization 'data / 255' \
    --output_tfjs \
    --output_coreml \
    --non_verbose
    mv saved_model/* ${MODEL}_${BATCH_SIZE}x16x2
    rm -rf saved_model
    mv ${MODEL}_${BATCH_SIZE}x16x2.onnx ${MODEL}_${BATCH_SIZE}x16x2
done


MODEL=pauseaug_mlp
for BATCH_SIZE in {1..10} ; do
    $INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
    --input_model ${MODEL}_${BATCH_SIZE}x16x2.onnx \
    --data_type FP32 \
    --output_dir ${MODEL}_${BATCH_SIZE}x16x2/openvino/FP32 \
    --model_name ${MODEL}_${BATCH_SIZE}x16x2
    $INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
    --input_model ${MODEL}_${BATCH_SIZE}x16x2.onnx \
    --data_type FP16 \
    --output_dir ${MODEL}_${BATCH_SIZE}x16x2/openvino/FP16 \
    --model_name ${MODEL}_${BATCH_SIZE}x16x2
    mkdir -p ${MODEL}_${BATCH_SIZE}x16x2/openvino/myriad
    ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/lib/intel64/myriad_compile \
    -m ${MODEL}_${BATCH_SIZE}x16x2/openvino/FP16/${MODEL}_${BATCH_SIZE}x16x2.xml \
    -ip U8 \
    -VPU_NUMBER_OF_SHAVES 4 \
    -VPU_NUMBER_OF_CMX_SLICES 4 \
    -o ${MODEL}_${BATCH_SIZE}x16x2/openvino/myriad/${MODEL}_${BATCH_SIZE}x16x2.blob
    openvino2tensorflow \
    --model_path ${MODEL}_${BATCH_SIZE}x16x2/openvino/FP32/${MODEL}_${BATCH_SIZE}x16x2.xml \
    --output_saved_model \
    --output_pb \
    --output_no_quant_float32_tflite \
    --output_dynamic_range_quant_tflite \
    --output_weight_quant_tflite \
    --output_float16_quant_tflite \
    --output_integer_quant_tflite \
    --output_integer_quant_typ 'uint8' \
    --string_formulas_for_normalization 'data / 255' \
    --output_tfjs \
    --output_coreml \
    --non_verbose
    mv saved_model/* ${MODEL}_${BATCH_SIZE}x16x2
    rm -rf saved_model
    mv ${MODEL}_${BATCH_SIZE}x16x2.onnx ${MODEL}_${BATCH_SIZE}x16x2
done


MODEL=pauseaug_stgcn
for BATCH_SIZE in {1..10} ; do
    mkdir -p ${MODEL}_${BATCH_SIZE}x16x2
    mv ${MODEL}_${BATCH_SIZE}x16x2.onnx ${MODEL}_${BATCH_SIZE}x16x2
done


MODEL=pauseaug_videopose
for BATCH_SIZE in {2..10} ; do
    $INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
    --input_model ${MODEL}_${BATCH_SIZE}x16x2.onnx \
    --data_type FP32 \
    --output_dir ${MODEL}_${BATCH_SIZE}x16x2/openvino/FP32 \
    --model_name ${MODEL}_${BATCH_SIZE}x16x2
    $INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
    --input_model ${MODEL}_${BATCH_SIZE}x16x2.onnx \
    --data_type FP16 \
    --output_dir ${MODEL}_${BATCH_SIZE}x16x2/openvino/FP16 \
    --model_name ${MODEL}_${BATCH_SIZE}x16x2
    mkdir -p ${MODEL}_${BATCH_SIZE}x16x2/openvino/myriad
    ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/lib/intel64/myriad_compile \
    -m ${MODEL}_${BATCH_SIZE}x16x2/openvino/FP16/${MODEL}_${BATCH_SIZE}x16x2.xml \
    -ip U8 \
    -VPU_NUMBER_OF_SHAVES 4 \
    -VPU_NUMBER_OF_CMX_SLICES 4 \
    -o ${MODEL}_${BATCH_SIZE}x16x2/openvino/myriad/${MODEL}_${BATCH_SIZE}x16x2.blob
    openvino2tensorflow \
    --model_path ${MODEL}_${BATCH_SIZE}x16x2/openvino/FP32/${MODEL}_${BATCH_SIZE}x16x2.xml \
    --output_saved_model \
    --output_pb \
    --output_no_quant_float32_tflite \
    --output_dynamic_range_quant_tflite \
    --output_weight_quant_tflite \
    --output_float16_quant_tflite \
    --output_integer_quant_tflite \
    --output_integer_quant_typ 'uint8' \
    --string_formulas_for_normalization 'data / 255' \
    --output_tfjs \
    --output_coreml \
    --non_verbose
    mv saved_model/* ${MODEL}_${BATCH_SIZE}x16x2
    rm -rf saved_model
    mv ${MODEL}_${BATCH_SIZE}x16x2.onnx ${MODEL}_${BATCH_SIZE}x16x2
done
